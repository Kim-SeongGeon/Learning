# Part 1. 수학의 기초

## Chapter 2. SLAM에 대한 첫 번쨰 지식

### 2.1 소개: 순무 로봇

이 책에서 소개하는 순무 로봇은 바퀴를 가지고 있기 때문에 움직일 수 있지만, 효과적인 경로 계획과 탐색 시스템 및 제어가 없다면 순무는 행동의 목표가 어디인지 알지 못한 채 맹목적으로 주변을 방황하는 것 외에는 아무것도 할 수 없게 된다. 이를 피하고자 먼저 주변 환경을 인식해야 하는데 로봇이 인간과 유사하다는 점에 착안하여 순무의 머리에 카메라를 설치했다. 순무가 방을 탐험할 수 있도록 하려면 최소한 다음의 두 가지를 알아야 한다.

1. 나는 지금 어디에 있을까? - Localization, 위치 추정
2. 내 주변 환경은 어떤 모습일까? - Map Building, 지도 작성

위치 추정과 지도 작성은 인지(perception)라는 영역의 각각 내부적 및 외부적 방향으로 볼 수 있다. 완전한 자율 이동 로봇으로서 순무는 내부적으로는 자신의 상태(즉, 위치)와 동시에 외부적으로는 주변 환경(즉, 지도)을 이해해야 한다.

레이저 센서, 카메라, 휠 엔코더(Wheel Encoder), 관성 측정 장치(Inertial Measurement Unit, IMU)와 같은 로봇 본체에 운반되는 센서는 일반적으로 직접 위치 데이터가 아닌 간접 물리적 양을 측정한다. 예를 들어, 휠 엔코더는 바퀴의 회전 각도를 측정하고 IMU는 움직임의 각속도와 가속도를 측정하며 카메라와 레이저 센서는 외부 환경을 이미지나 포인트 클라우드와 같은 특정 형태로 관측한다.

Visual SLAM이 이 책의 주제이므로, 우리는 특히 순무의 눈(즉, 카메라)이 무엇을 할 수 있는지에 대해 관심이 있다. SLAM에 사용되는 카메라는 우리가 일반적으로 보는 SLR(Single Lens Reflex) 카메라와 동일하지 않다. 

작동 방식에 따라 카메라는 단안 카메라, 스테레오 카메라 및 RGB-D 카메라의 세가지 범주로 나눌 수 있다. 단안 카메라는 직관적으로 알 수 있듯이 하나의 카메라를 가지고 있으며 스테레오 카메라는 두 개의 카메라를 가지고 있다. 반면, RGB-D 카메라의 원리는 조금 더 복잡한데 컬러 사진을 캡처할 수 있을 뿐만 아니라 픽셀별로 카메라로부터 캡처된 대상까지의 거리를 측정할 수 있다. 그래서 RGB-D 카메라는 깊이 카메라라고도 불린다. 

### 2.1.1 단안 카메라

SLAM을 위해 하나의 카메라만 사용하는 것을 단안 SLAM(Monocular SLAM)이라고 한다. 우리는 단안 카메라의 데이터를 본 적이 있다: 그렇다, 물론 사진이다. 사진은 기본적으로 장면(Scene)이 카메라 이미징 평면에 남긴 투영을 촬영하는 것이다. 그것은 3차원 세계를 2차원 이미지의 형태로 기록하는 것이다. 그렇다면 분명히 이 사진을 찍는 프로세스는 깊이(또는 거리)라고 하는 장면의 차원 중 하나를 잃어버리게 되는 것이다. 단안 카메라에서는 장면의 피사체와 카메라 사이의 거리를 단일 이미지로는 얻을 수 없다. 단안 카메라로 촬영한 영상은 3차원 공간의 2차원 투영일 뿐이기 때문에 3차원 구조를 복원하려면 카메라의 시야각을 바꿔야 한다. 단안 SLAM에서도 동일한 원리가 적용된다. 단안 SLAM에 의해 추정된 궤적 및 지도가 소위 스케일(Scale)이라는 요소에 의해 실제 궤적 및 지도와 다를 것임을 의미한다. 단안 SLAM은 이미지만으로는 실제 스케일을 결정할 수 없으며 이를 스케일 모호성(Scale Ambiguity)이라고도 한다.

단안 SLAM에서 깊이 정보는 이동 후에만 계산할 수 있고 실제 축척을 결정할 수 없으므로, 단안 SLAM을 실제 세계에 적용하는 것은 큰 어려움이 있었다. 위에서도 언급했듯이 근본 원인은 하나의 이미지에서 깊이를 결정할 수 없기 때문이다. 그래서 사람들은 이 깊이 정보를 얻기 위해 스테레오(양안 또는 쌍 안) 카메라와 RGB-D 카메라를 사용하기 시작했다.

### 2.1.2 스테레오 카메라 및 RGB-D 카메라

스테레오 카메라와 RGB-D 카메라를 사용하는 목적은 물체와 카메라 사이의 거리를 측정하여 단안 카메라가 거리를 알 수 없다는 단점을 극복하는 것이다. 거리가 알려지면 스케일 불확실성을 제거하면서 장면의 3D 구조를 복구할 수 있다. 둘 다 거리를 측정하는 데 사용되지만, 스테레오 카메라와 RGB-D 카메라는 거리를 측정하는 원리가 다르다. 스테레오 카메라는 두 대의 단안 카메라로 구성된 양안 또는 쌍 안 카메라라고도 하며 이 두 카메라 사이의 거리인 기준선(Baseline)이 알고 있는 값이다. 우리는 이 두 카메라 사이의 거리를 사용하여 사람의 눈과 매우 유사하게 각 픽셀의 공간적 위치를 추정한다. 

스테레오 카메라는 각 픽셀의 깊이를(신뢰하기 힘들 정도로라도) 추정하기 위해 많은 계산을 해야 하며 그 성능은 인간보다 정말 서툴다고 말할 정도이다. 스테레오 카메라로 측정한 깊이 범위는 두 카메라 사이의 거리인 기준선의 길이와 관련이 있다. 기준선의 길이가 길수록 물체를 더 멀리 측정할 수 있으므로 무인 자동차의 양안 카메라는 일반적으로 꽤 클 수밖에 없다. 스테레오 카메라의 거리 추정은 좌안과 우안의 영상을 비교하여 얻어지며, 다른 센싱 장치에 의존하지 않으므로 실내외에 모두 적용할 수 있다. 양안 또는 다 안 카메라의 단점은 구성 및 보정이 복잡하고 깊이 범위와 정확도가 양안 기준선 및 해상도에 의해 제한되며 시차 계산에 컴퓨팅 리소스가 많이 소모된다는 점이다. 그래서 일반적으로 실시간으로 깊이 지도를 생성하기 위해서는 GPU나 FPGA를 이용한 가속이 필요하다.

RGB-D 카메라(또는 깊이 카메라라고도 하는데 이 책에서 주로 RGB-D 카메라라는 이름을 사용)의 가장 큰 특징은 적외선 또는 ToF(Time-of-Flight)의 원리를 이용하여 물체와의 거리를 계산한다는 것이다. 원리는 레이저 센서와 마찬가지로 물체에 능동적으로 빛을 방출하고 반환된 빛을 수신하여 물체와 카메라 사이의 거리를 측정하는 것이다. 스테레오 카메라처럼 소프트웨어 게산으로 해결되는 것이 아니라 물리적인 측정 방식으로 해결하기 때문에 스테레오 카메라에 비해 많은 컴퓨팅 자원을 절약할 수 있다. 그러나 대부분의 RGB-D 카메라는 여전히 좁은 측정 범위, 높은 노이즈, 작은 화각(FoV, Field of View), 태양광의 간섭을 받기 쉬운 점, 그리고 투명한 물체는 측정이 불가한 점 등 많은 문제를 가지고 있으므로 SLAM 목적으로는 실외에서 사용이 제한되는 측면이 있어 주로 실내에서만 사용된다.

### 2.2 전형적인 Visual SLAM 프레임워크

↓ 전형적인 Visual SLAM 프레임워크 이미지

<img src="Images/Typical Visual SLAM Framework.png" width="500"/>

전체 Visual SLAM 프로세스에는 다음 단계가 포함된다.

1. 센서 정보를 읽는다. Visual SLAM에서는 주로 카메라 이미지 정보를 읽고 전처리를 한다. 로봇의 경우 모터 엔코더 및 관성 센서와 같은 정보를 읽고 동기화할 수도 있다.

2. Visual Odometry는 시각적 주행거리 측정이라고도 하는데 인접한 이미지 사이의 카메라 움직임을 추정하고 대강의 로컬 맵을 생성하는 것이며 프론트엔드(Frontend)라고도 한다.

3. Filters Optimization은 비선형 최적화 과정으로 서로 다른 시간에 VO로 측정한 카메라 포즈와 루프 백 감지 정보를 받아들이고 최적화하여 전역적으로 일관된 궤적과 지도를 얻는 것을 말한다. VO 뒤에 연결되기 때문에 백엔드(Backend)라고도 한다.

4. Loop Closing은 루프 백 감지라고 하며 로봇이 이전에 도달했던 위치에 다시 왔는지 여부를 결정한다. 루프 백이 감지되면 더 나은 최적화 처리를 위해 정보를 백엔드에 제공한다.

5. Reconstruction은 매핑 즉, 맵을 만드는 과정으로 추정된 카메라의 궤적을 기반으로 임무 요구사항에 해당하는 맵을 구축하는 것을 말한다.

### 2.2.1 Visual Odometry

Visual Odometry는 시각적 주행거리 측정(이후에는 VO로 표현)이며 인접한 이미지 간의 카메라 움직임을 고려하는 것이다.

컴퓨터 비전 부야에서는 인간에게 직관적으로 자연스러운 일들이 컴퓨터는 매우 어렵다. 이미지는 컴퓨터 입장에서 숫자 행렬일 뿐이다. 이 행렬에 표현된 것이 어떤 것을 의미하는지 컴퓨터 입장에서는 알 수 없다.(이것을 이제 머신 러닝이 해결하고 있다.) Visual SLAM에서는 픽셀 단위로만 볼 수 있으며, 이는 카메라의 이미징 평면에 공간 포인트를 투영한 결과라는 것을 우리는 알고 있다. 따라서 카메라의 움직임을 정량적으로 추정하기 위해서는 먼저 카메라와 공간적 점 사이의 기하학적 관계에 대한 이해가 필요하다.

이 기하학적 관계와 VO의 구현을 명확히 하려면 몇 가지 배경 지식이 필요하다. 우리는 VO가 인접한 프레임 사이의 이미지에서 카메라 움직임을 추정하고 장면의 공간 구조를 복구할 수 있다는 사실만 알면 된다. 실제 주행거리 측정계와 마찬가지로 인저한 순간의 움직임만 계산하고 과거 정보와 상관관계가 없으므로 "주행 거리계"라고 하는 것이다. 이떄 VO는 단기기억만 있다고 생각할 수 있다.

시각적 주행거리 측정은 실제로 SLAM의 핵심이지만 시각적 주행거리 측정만으로 궤적을 추정하는 것은 필연적으로 축적된 누적 오차(Accumulating Drift)로 이어질 것이다. 이는 VO가 (가장 단순한 경우) 두 이미지 간의 움직임만 추정한다는 사실 때문이다. 각 추정에는 어느 정도의 오차가 존재하며, 주행 거리계가 작동하는 방식으로 인해 이전 순간의 오차가 다음 순간으로 전달되어 일정 시간이 지나면 예상 궤적이 더 이상 정확하지 않게 된다. 

이와 같은 현상을 드리프트(Drift)라고도 하며 일관된 지도를 구축하는 데 방해가 된다. 이와 같은 드리프트 문제를 해결하려면 백엔드 최적화(보통 그냥 백엔드라고 한다.)와 루프 백 감지라는 두 가지 기술이 더 필요하다. 루프 백 감지는 "로봇이 원래 위치로 돌아감"을 감지하는 역할을 하며, 백엔드 최적화는 이 정보를 기반으로 전체 궤적의 모양을 수정하는 것을 말한다.

### 2.2.2 백엔드 최적화

일반적으로 백엔드 최적화는 주로 SLAM 프로세스에서 노이즈 문제를 처리하는 것을 말한다. 모든 데이터가 정확하기를 바라지만 실제로는 가장 정확한 센서라도 일정량의 노이즈가 있다. 일부 센서는 자기장과 온도의 영향을 받기도 한다. 따라서 "이미지에서 카메라 움직임을 추정하는 방법"을 해결하는 것 외에도 이 추정치가 얼마나 노이즈가 많은지 이 노이즈가 어느 한순간에서 다음 순간으로 어떻게 전달되는지 현재 추정치의 신뢰도가 얼마나 큰지에 관해서도 관심을 둔다. 백엔드 최적화에서 고려해야 할 문제는 이러한 잡음이 있는 데이터에서 전체 시스템의 상태를 추정하는 방법과 상태 추정이 얼마나 불확실한가이다. 이를 최대 사후 확률 추정(Maximum-A-Posteriori, MAP)이라고 한다. 여기서 상태에는 로봇의 포즈 추정과 지도가 모두 포함된다.

대조적으로, Visual Odometry 부분은 때때로 "프론트엔드"라고 한다. SLAM 프레임워크에서 프론트엔드는 백엔드에 최적화할 데이터와 이러한 데이터의 초깃값을 제공한다. 백엔드는 전체 최적화 프로세스를 담당하기 때문에 데이터만 마주하는 경우가 많으며 데이터가 어느 센서에서 오는지 신경 쓸 필요가 없다. 다른 말로 백엔드에서는 이미지 대신에 단지 숫자와 행렬 데이터만 존재한다. 그러므로 Visual SLAM에서 프론트엔드는 이미지 특징 추출 및 매칭과 같은 컴퓨터 비전 연구 분야와 더 관련이 있으며 백엔드는 주로 상태 추정, 필터링 및 비선형 최적화 알고리즘과 관련이 있다.

초기 SLAM 문제는 대부분 상태 추정 문제였으며 정확히는 백엔드 최적화가 해결해야 하는 문제였다. SLAM을 처음 제안한 일련의 논문에서 당시 사람들은 이를 "공간적 불확실성"이라고 불렀다. 다소 모호하지만, SLAM 문제의 본질은 움직이고 있는 카메라(물체)와 주변 환경의 공간적 불확실성 추정이라고 할 수 있는 것이다. 따라서 SLAM 문제를 해결하기 위해서는 위치 추정과 매핑의 불확실성을 표현한 다음, 필터나 비선형 최적화를 사용하여 상태의 평균과 불확실성(분산)을 추정하는 상태 추정 이론이 필요하다.

### 2.2.3 루프 백 감지

루프 폐쇄 감지라고도 하는 루프 백 감지는 주로 시간 경과에 따른 위치 추정의 드리프트 문제를 해결한다. 실제로 로봇이 일정 시간 이동 후 원점으로 돌아왔을 때 드리프트로 인해 위치 추정치가 원점으로 돌아오지 않는 현상을 말한다. 로봇이 "원점으로 돌아갔다"라는 것을 알 수 있거나 "원점"을 식별할 방법이 있다면 위치 추정치를 원점으로 "끌어오는 것"으로 드리프트를 제거할 수 있을 것이며 이를 루프 백 감지라고 한다.

루프 백 감지는 "위치" 및 "맵 빌딩"과 밀접하게 관련되어 있다. 루프 백을 감지하려면 로봇이 이전에 본 장면을 인식할 수 있는 기능을 제공해야 한다. 우리는 루프 백 감지를 카메라 이미지 자체만으로 수행할 수 있어야 한다. 루프 백 감지에 성공하면 누적 오차를 크게 줄일 수 있다. 루프 백 감지는 이미지 간의 유사성을 판단하여 수행할 수 있다. 따라서 Visual SLAM의 루프 백 감지는 본질적으로 이미지 데이터의 유사성을 계산하기 위한 알고리즘이다.

루프 백을 감지한 후 백엔드 최적화 알고리즘에 "A와 B는 같은 지점이다."와 같은 내용을 알린다. 그런 다음 백엔드는 이 새로운 정보를 기반으로 루프 백 감지 결과와 일치하도록 궤적과 지도를 조정하는 작업을 한다. 이러한 방식으로 적절하고 정확한 루프 백 감지가 있으면 누적된 오류를 제거하고 전역적으로 일관된 궤적과 지도를 얻을 수 있다.

### 2.2.4 매핑

매핑은 지도를 만드는 과정을 의미한다. 지도는 환경에 대한 묘사이지만 묘사하는 방법은 고정되어 있지 않으며 SLAM 적용 분야에 따라 다르다.

지도의 경우 매우 많은 아이디어와 요구사항이 있다. 따라서 Visual Odometry, 백엔드 최적화 및 루프 백 감지에 비해 매핑을 위한 고정된 형식과 알고리즘은 없다. 단순히 3차원 공간상의 점들에 모음을 지도라고 부를 수 있고 텍스처 정보를 포함한 3D 모델도 지도이며 도시, 마을, 철도, 강을 나타내는 그림이나 사진들도 지도이다. 즉, 지도의 형태는 SLAM의 적용에 따라 다른데 일반적으로 메트릭 맵과 토폴로지 맵의 두 가지 유형으로 나눌 수 있다.

### 메트릭 맵(Metric Map)

메트릭 맵은 지도에서 객체의 위치 관계를 정확하게 나타내는 것을 강조하며 일반적으로 희소(Sparse)와 조밀(Dense)로 분류된다. 희소맵은 특정 수준의 추상화를 제공하며 모든 개체를 나타낼 필요가 없는 지도를 말한다. 예를 들어, 어떤 것 중 대표적인 의미가 있는 부분을 선택하여 랜드마크(Landmark)라고 하고 이 랜드마크들로 구성된 지도가 희소맵이며 랜드마크 이외의 부분은 무시할 수 있다. 대조적으로 조밀맵은 보이는 모든 것을 모델링하는 데 중점을 둔다. 위치 추정에는 희소맵으로 충분하다. 반면, 내비게이션을 위해서는 밀도가 높은 조밀맵이 필요한 경우가 많다(그렇지 않으면 두 랜드마크 사이의 벽에 부딪힐 수 있다). 

조밀맵은 일반적으로 특정 해상도에 따라 많은 작은 그리드(Grid)로 구성되며(2차원 메트릭 맵에서), 3차원 메트릭 맵에서는 많은 작은 사각형인 복셀(Voxel)로 표현된다. 일반적으로 각각 격자에는 점유, 점유되지 않음, 알 수 없음의 세 가지 상태가 존재하며 해당 격자에 개체가 있는지를 나타낸다. 특정 공간 위치를 문의할 때 지도는 해당 위치가 통과 가능한지에 대한 정보를 제공할 수 있다. 이러한 지도는 A*, D* 등과 같은 다양한 탐색 알고리즘에 사용할 수 있다. 그러나 한편으로 이러한 종류의 맵은 각 격자 포인트의 상태를 저장해야 하므로 많은 저장 공간을 소비하고 대부분 경우 맵의 많은 세부 정보가 쓸모가 없다는 것도 알 수 있다. 반면에 대규모 메트릭 맵에는 일관성 문제가 있는 경우가 있다. 약간의 오차들이 누적된다면 예를 들어, 두 방의 벽이 겹치게 되어 지도가 무용지물이 될 수도 있다.

### 토폴로지 맵(Topological Map)

비교적 정확한 메트릭 맵과는 달리 토폴로지 맵은 지도 요소 간의 관계를 강조한다. 또한 토폴로지 맵은 노드와 에지로 구성된 그래프(Graph)로 노드 간의 연결성만 고려한다. 예를 들면 점 $A$와 $B$의 연결성에만 주목하고 점 $A$와 $B$까지 어떻게 가는지는 고려하지 않으며, 정확한 위치에 대한 지도의 필요성을 완화하고 지도의 세부 정보를 제거한 보다 간결한 표현 방식이다. 그러나 토폴로지 맵은 복잡한 구조의 맵을 잘 표현하지 못한다. 노드와 에지로부터 맵을 분할하는 방법과 탐색 및 경로 계획을 위해 토폴로지 맵을 사용하는 방법은 여전히 연구해야 할 문제이다.

## 2.3 SLAM 문제의 수학적 공식화

다시 순무 로봇으로 돌아와서 순무가 미지의 환경에서 움직일 수 있는 일종의 센서를 가지고 있다고 가정할 때 이것을 수학적 언어로 어떻게 설명해야 할까? 첫째, 카메라는 보통 특정 순간에 데이터를 수집하기 때문에 우리는 이 순간의 위치와 지도에만 신경을 쓴다. 이것은 연속 시간 운동의 기간을 불연속적인 순간 $t = 1, ... , k$에서 발생하는 것으로 바꾼다. 이때 $x$를 사용하여 순무 위치를 나타낸다. 따라서 매 순간의 위치는 순무의 궤적을 구성하는 $x_1, ... , x_k$로 기록된다. 또한 지도가 여러 랜드마크들로 구성되어 있다고 가정하고 매 순간 센서가 랜드마크의 일부를 측정하고 관찰 데이터를 얻는다. 총 $N$개의 랜드마크가 있다면 $y_1, ... , y_N$으로 수학적인 표현을 할 수 있을 것이다.

먼저 모션을 살펴보면, 일반적으로 로봇은 휠 엔코더나 관성 센서와 같이 자체 동작을 측정하는 센서를 가지고 있다. 이 센서는 움직임에 대한 판독 값을 측정할 수 있으며, 위치의 차이는 물론 가속도, 각속도 등의 정보도 측정할 수 있다. 

$$x_k = f(x_{k-1}, u_k, w_k)$$

여기서 $u_k$는 입력값이고 $w_k$는 이 과정에서 추가된 노이즈이다. $f$는 어떤 특정한 작동하는 방식을 지정하는 것은 아니고 이 프로세스를 설명하기 위한 일반 함수 $f$를 말한다. 이를 통해 전체 기능이 특정 센서에 국한되지 않고 모든 모션 센서와 입력을 참조할 수 있는 일반 방정식이 되며 우리는 이것을 모션 모델이라고 부른다.

노이즈의 존재는 이 모델을 스토캐스틱(Stochastic, 확률적인)한 모델로 만든다. 즉, '1미터 전진' 명령을 내린다고 해서 실제로 순무가 1미터 전진하는 것은 아니다. 모든 명령이 완전히 정확하다면 우리는 추정이라는 과정을 거칠 필요가 없다. 따라서 각 이동 중 노이즈는 랜덤하다. 만약 이 노이즈를 무시하고 동작 명령에 의해서만 결정된 위치는 실제 위치에서 수십 킬로미터 떨어져 있을 수도 있다.

두 번째 관찰에 해당하는 모델방정식도 있다. 관찰 모델은 순무가 $x_k$의 위치에서 특정 랜드마크 $y_j$를 볼 때 관찰 데이터 $z_{k,j}$가 생성된다는 것을 나타낸다. 모션 모델에서 추상 함수 $f$를 사용하여 표현했다면 관찰 모델에서는 추상 함수 $h$를 사용하여 이 관계를 설명한다.

$$z_{k,j} = h(y_j, x_k, v_{k,j})$$

여기서 $v_{k,j}$가 이 관찰에서 노이즈이다. 관찰에 사용되는 센서의 형태는 무수히 많으므로 여기에서 관찰 데이터 $z$와 관찰 방정식 $h$도 다양한 형태를 가진다.

그리고 여기서 $x,y,z$는 순무의 실제 움직임과 센서의 종류에 따라 달라지는 것을 매개변수화(Parameterization)한 방법이다. 매개변수화란 예를 들어, 순무가 평면에서 이동한다고 가정하면 순무의 포즈는 $x$와 $y$ 좌표 평면에서의 위치와 회전 각도 $\theta$, 즉 $x_k = [x_1, x_2, \theta]_k^{T}$
라고 나타낼 수 있다. 동시에 모션에 대한 입력 명령은 두 시간 간격의 위치와 회전 각도의 변화 $u_k = [\Delta x_1, \Delta x_2, \Delta \theta]_k^{T}$ 이므로 운동 방정식은 다음과 같이 구현될 수 있다.

$$\begin{bmatrix} x_1 \\ x_2 \\ \theta \end{bmatrix}^T_k = \begin{bmatrix} x_1 \\ x_2 \\ \theta \end{bmatrix}^T_{k-1} + \begin{bmatrix} \Delta x_1 \\ \Delta x_2 \\ \Delta \theta \end{bmatrix}^T_k + w_k $$

$w_k$는 노이즈이며 위의 식은 단순한 선형 관계이다. 그러나 모든 입력 명령이 변위와 각도의 변화는 아니다. 예를 들어, "액셀러레이터" 또는 "컨트롤 스틱"의 입력은 속도 또는 가속도이므로 더 복잡한 다른 형태의 모션 방정식이 있다. 따라서 운동 분석이 필요하다.

관찰 모델방정식과 관련하여 순무에 의해 운반되는 2차원 레이저 센서를 예를 들어 보면, 레이저 센서가 2D 랜드마크를 관찰할 때 랜드마크와 순무 몸체 사이의 거리 $r$과 각도 $\phi$의 두 가지 양을 측정할 수 있다는 것을 알고 있다. 랜드마크 점 $y_j = [y_1, y_2]^T_j$을 순무의 위치 $x_k = [x_1, x_2]^T_k$에서 관찰한 데이터를 $z_{k,j} = [r_{k,j}, \phi_{k,j}]^T$라고 표시하면 관찰 모델방정식은 다음과 같이 작성된다.

$$\begin{bmatrix} r_{k,j} \\ \phi_{k,j} \end{bmatrix}^T = \begin{bmatrix} \sqrt{(y_{1,j} - x_{1,k})^2 + (y_{2,j} - x_{2,k})^2} \\ \arctan\left(\frac{y_{2,j} - x_{2,k}}{y_{1,j} - x_{1,k}}\right) \end{bmatrix}^T + v$$

